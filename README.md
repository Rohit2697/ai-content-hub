integrate with Prisma client 
1. npm install prisma
2. npx prisma init --datasource-provider sqlite
3. define model
ex: prisma\schema.prisma
model Post {
  id          Int    @id @default(autoincrement())
  slug        String
  title       String
  description String
  content     String
  tags        String
  author      String
  date        String
  readingTime String
  coverImage  String
}

4. npx prisma migrate dev

‚úÖ Goal Recap
Let the user:

Enter a prompt (e.g., ‚ÄúWrite an article on AI in healthcare‚Äù)

Generate: title, tags, description, and content

Support follow-up prompts (e.g., ‚ÄúMake it longer‚Äù or ‚ÄúAdd references‚Äù)

Use Ollama (Mistral) and Langchain with memory

üîß Step-by-Step Plan
1. Setup LangChain with Ollama (Mistral)
Install dependencies:

bash
Copy
Edit
npm install langchain @langchain/community
Use ChatOllama:

ts
Copy
Edit
import { ChatOllama } from '@langchain/community/chat_models/ollama';

const llm = new ChatOllama({
  baseUrl: 'http://localhost:11434',
  model: 'mistral',
});
2. Add Memory Support
Choose memory backend:

‚úÖ Best for persistence: SQLite via @langchain/community

For simplicity: In-memory store (BufferMemory)

For SQLite:

bash
Copy
Edit
npm install sqlite3
Use memory with LangChain:

ts
Copy
Edit
import { ChatMessageHistory } from "langchain/stores/message/sqlite";
import { BufferMemory } from "langchain/memory";

const messageHistory = new ChatMessageHistory({
  databasePath: './memory.db',
  sessionId: 'user-session-id',
});

const memory = new BufferMemory({
  chatHistory: messageHistory,
  returnMessages: true,
  memoryKey: 'chat_history',
});
3. Create LangChain Agent / Chain
Use a simple prompt template:

ts
Copy
Edit
import { ConversationChain } from 'langchain/chains';
import { PromptTemplate } from 'langchain/prompts';

const prompt = PromptTemplate.fromTemplate(`
You are a helpful assistant for writing blog articles.
User: {input}
`);

const chain = new ConversationChain({
  llm,
  memory,
  prompt,
});
4. Call the Chain from PostArticle Page
Create an endpoint (e.g., /api/generate-article) that receives prompt input and returns title, tags, etc.

Example call:

ts
Copy
Edit
const response = await chain.call({ input: "Write an article on AI in healthcare" });
// Use response.text to parse or split into title, tags, etc.
You can split the response using structured prompts or by asking the LLM:

‚ÄúReturn the response as a JSON with fields: title, description, tags, content.‚Äù

5. UI Integration (PostArticle Page)
Textarea or input for prompt

A ‚ÄúGenerate Article‚Äù button

Show loading spinner

Display the generated content in editable fields

Optional enhancement:

A chat-like UI for iterative prompts

üì¶ Bonus Tip: Use Output Parsers
To structure LLM responses cleanly:

ts
Copy
Edit
import { StructuredOutputParser } from "langchain/output_parsers";

const parser = StructuredOutputParser.fromNamesAndDescriptions({
  title: "The article title",
  tags: "Relevant keywords",
  description: "A short summary",
  content: "The full article",
});

const prompt = new PromptTemplate({
  template: `Generate an article with this input: {input}. Return as JSON.\n{format_instructions}`,
  inputVariables: ['input'],
  partialVariables: {
    format_instructions: parser.getFormatInstructions(),
  }
});
Then parse the result:

ts
Copy
Edit
const result = await chain.call({ input: userPrompt });
const parsed = await parser.parse(result.response);
Would you like me to scaffold a working /api/generate-article endpoint with memory and parsing for you?